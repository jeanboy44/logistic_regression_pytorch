{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression with sklearn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. load data\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "X = X[:,1:3]\n",
    "\n",
    "# 2. Split Train and Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=5\n",
    ")\n",
    "\n",
    "# 3. Fit model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(reg.coef_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE: 4392.632885400543\n",
      "[  2.33410605 938.42795975]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression from Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Components\n",
    "### 1. Prediction\n",
    "### 2. Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction\n",
    "### Mathematical formula\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y=\\theta^Tx\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "h_{\\theta}(x)=\\theta^Tx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- notations\n",
    "    - $\\theta$: weight vector(D x 1 dim.)\n",
    "    - **x**: input(1 row) vector(D x 1 dim.)\n",
    "    - D: the number of features + 1\n",
    "- 위 둘은 같은 식\n",
    "- 참고\n",
    "    - 보통 선형회귀식은 intercept 항을 따로 빼서 표기하기지만, 이후 수식 전개 등을 쉽게 하기 위해 theta0가 intercept라고 둠."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "N = X_train.shape[0]\n",
    "D = X_train.shape[1]+1\n",
    "theta = np.random.normal(0, 1, D).reshape(-1,1)\n",
    "X = np.concatenate([np.ones(X_train.shape[0]).reshape(-1,1), X_train], axis=1) # Add bias term\n",
    "x = X_train[0,:]\n",
    "x = np.concatenate([[1], x]).reshape(-1,1) # add 1 for intercept \n",
    "\n",
    "# using for loop\n",
    "prediction = 0\n",
    "for i in range(D):\n",
    "    prediction += theta[i][0]*x[i][0]\n",
    "    prediction = prediction\n",
    "\n",
    "# vectorized\n",
    "prediction = theta.T.dot(x)\n",
    "\n",
    "# Predict multiple data\n",
    "predictions = X.dot(theta)\n",
    "\n",
    "def predict(X, theta):\n",
    "    X = np.concatenate([np.ones(X.shape[0]).reshape(-1,1), X], axis=1) # Add bias term\n",
    "    y_hat = X.dot(theta)\n",
    "    \n",
    "    return y_hat.flatten()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "### Cost function(= Loss function)\n",
    "#### Cost function of linear regression: Mean Squared Error\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{N}\\sum_{i=1}^N(h_{\\theta}(x^{(i)})-y^{(i)})^2\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{N}\\sum_{i=1}^N((\\theta^Tx^{(i)}+b)-y^{(i)})^2\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{N}(X\\theta-y)^T(X\\theta-y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- notations\n",
    "    - **$\\theta$**: weight vector(D x 1 dim.)\n",
    "    - **x**: input(1 row) vector(D x 1 dim.)\n",
    "    - **X**: input matrix(N x D dim.)\n",
    "    - **y**: output vector(N x 1 dim.)\n",
    "    - D: the number of features\n",
    "- 위 세 개는 전부 같은 식\n",
    "- MAE를 최소가 되게 하는 **w**를 찾는다!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# cost function\n",
    "def mse(y_hat, y):\n",
    "    N = len(y)\n",
    "    cost = sum((y_hat-y)**2)/N\n",
    "    return cost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimization Method\n",
    "- 위에 정의한 cost function을 최대화 하기위해서는 어떠한 과정이 필요한가?\n",
    "- 선형회귀의 경우, 미분을 사용해서 해를 구할 수 있음!\n",
    "- 단, 일반적으로는 수리적인 방법(Ex. 동전던지기의 확률을 구한 방법, 미분 등)으로 해를 구할 수 없음\n",
    "- Optimization Method의 대표적인 예: **Gradient descent**\n",
    "    - Mathematical Formula\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta \\leftarrow \\theta - \\eta{\\nabla}_{\\theta}J\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\nabla}_{\\theta}J=\\sum_{n=1}^Nx^{(i)}(h_{\\theta}(x^{(i)})-y^{(i)})\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\nabla}_{\\theta}J=\\frac{1}{N}X^T(X\\theta-y)\n",
    "\\end{align}\n",
    "$$\n",
    "</br>\n",
    "$$\n",
    "h_{\\theta}(x^{(i)}) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2+...+ \\theta_Dx_D\n",
    "$$\n",
    "\n",
    "- notations\n",
    "    - $\\eta$ : learning rate\n",
    " \n",
    "<img src=\"../figures/GradientDescentGIF.gif\" alt=\"drawing\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "--------------------\n",
    "\n",
    "## 조금 더 깊게\n",
    "\n",
    "- 기계학습의 “학습”은 단순히 모델의 가중치(w)를 찾아내는 것\n",
    "    - 비유하자면, 새로운 기억이 생성될 때마다, 뇌에 있는 각 시냅스 간의 연결의 세기가 변한다!\n",
    "- 이러한 관점에서, 기계학습 문제는 단순히 주어진 데이터(X, y)를 가장 잘 설명하는 가중치를 찾아내는 것이다.\n",
    "- 이러한 가중치를 찾아내는 방법 중 가장 많이 사용되는 것이 최대우도추정(Maximum likelihood Estimation) 방법이다. \n",
    "\n",
    "### Likelihood?\n",
    "<!-- ![likelihoood](../figures/likelihood2.png) -->\n",
    "<img src=\"../figures/likelihood2.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "### Base theorem\n",
    "<img src=\"../figures/baise_theorem.png\" alt=\"drawing\" width=\"400\"/>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Gradient Descent 함수 생성\n",
    "def gradientDescent(X, y, theta, alpha, N, numIterations, verbose=1):\n",
    "    X_tmp = X.copy()\n",
    "    X_tmp = np.concatenate([np.ones(X_tmp.shape[0]).reshape(-1,1), X_tmp], axis=1) # Add bias term\n",
    "#     tmp = (tmp - tmp.mean()) / (tmp.max() - tmp.min()) # standardization for computing convinience\n",
    "    # bias 추가\n",
    "    for i in range(0, numIterations):\n",
    "        # Predict\n",
    "        hypothesis = X_tmp.dot(theta)\n",
    "        loss = hypothesis.reshape(-1,1) - y.reshape(-1,1)\n",
    "        # avg cost per example (the 2 in 2*n doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / N\n",
    "        if verbose==1:\n",
    "            if(i%100000==0):\n",
    "                print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = X_tmp.T.dot(loss) / N\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression with Numpy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def predict(X, theta):\n",
    "    X = np.concatenate([np.ones(X.shape[0]).reshape(-1,1), X], axis=1) # Add bias term\n",
    "    y_hat = X.dot(theta)\n",
    "    \n",
    "    return y_hat.flatten()\n",
    "\n",
    "# Gradient Descent 함수 생성\n",
    "def gradientDescent(X, y, theta, alpha, N, numIterations, verbose=1):\n",
    "    X_tmp = X.copy()\n",
    "    X_tmp = np.concatenate([np.ones(X_tmp.shape[0]).reshape(-1,1), X_tmp], axis=1) # Add bias term\n",
    "#     tmp = (tmp - tmp.mean()) / (tmp.max() - tmp.min()) # standardization for computing convinience\n",
    "    # bias 추가\n",
    "    for i in range(0, numIterations):\n",
    "        # Predict\n",
    "        hypothesis = X_tmp.dot(theta)\n",
    "        loss = hypothesis.reshape(-1,1) - y.reshape(-1,1)\n",
    "        # avg cost per example (the 2 in 2*n doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / N\n",
    "        if verbose==1:\n",
    "            if(i%100000==0):\n",
    "                print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = X_tmp.T.dot(loss) / N\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta\n",
    "\n",
    "# 1. load data\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "X = X[:,1:3]\n",
    "D = X_train.shape[1]+1\n",
    "\n",
    "# 2. Split Train and Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=5\n",
    ")\n",
    "\n",
    "# 3. Initialize model\n",
    "# - Set hyper parameters\n",
    "epochs= 500000\n",
    "learning_rate = 0.005\n",
    "# - Initialize weight\n",
    "theta = np.random.normal(0, 1, D).reshape(-1,1) #Initalize theta\n",
    "\n",
    "# 4. Fit model\n",
    "theta = gradientDescent(X=X_train,\n",
    "                        y=y_train,\n",
    "                        theta=theta,\n",
    "                        alpha=learning_rate,\n",
    "                        N=X_train.shape[0],\n",
    "                        numIterations=epochs,\n",
    "                        verbose=1)\n",
    "# 4. Predict\n",
    "y_pred = predict(X_test, theta)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 0 | Cost: 28779.649938\n",
      "Iteration 100000 | Cost: 3963.942000\n",
      "Iteration 200000 | Cost: 3784.199511\n",
      "Iteration 300000 | Cost: 3766.496319\n",
      "Iteration 400000 | Cost: 3764.711608\n",
      "MSE: 4393.753715842214\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(\"Compare coefficients. sklearn vs. mine\")\n",
    "print(pd.DataFrame({\"sklearn\":np.concatenate([[reg.intercept_],reg.coef_.flatten()]), \"mine\":theta.flatten()}))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Compare coefficients. sklearn vs. mine\n",
      "      sklearn        mine\n",
      "0  151.812188  151.810914\n",
      "1    2.334106    3.472117\n",
      "2  938.427960  935.514359\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression with Pytorch 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def predict(X, theta):\n",
    "    hypothesis = torch.matmul(X, theta)\n",
    "    \n",
    "    return hypothesis\n",
    "\n",
    "# 1. load data\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "X = X[:,1:3]\n",
    "X = np.concatenate([np.ones(X.shape[0]).reshape(-1,1), X], axis=1) # Add bias term\n",
    "\n",
    "# 2. Split Train and Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=5\n",
    ")\n",
    "N = X_train.shape[0]\n",
    "D = X_train.shape[1]\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "# 3. Initialize model\n",
    "# - Set hyper parameters\n",
    "epochs= 500000\n",
    "learning_rate = 0.005\n",
    "# - Initialize weight\n",
    "theta = torch.zeros(D, requires_grad=True).float()\n",
    "# - Set optimizezr\n",
    "optimizer = optim.SGD([theta], lr=learning_rate) # Stochastic Gradient Descenct\n",
    "\n",
    "# 4. Fit model\n",
    "for epoch in range(epochs):\n",
    "    # H(x) 계산\n",
    "    hypothesis = torch.matmul(X_train, theta)\n",
    "#     hypothesis = torch.dot(X_train,theta.view(-1,1))\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100000 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} Cost: {cost.item()}\")\n",
    "        \n",
    "# 5. Predict\n",
    "y_pred = predict(X_test, theta)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred.detach().numpy())}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/500000 Cost: 28527.291015625\n",
      "Epoch 100000/500000 Cost: 3784.14306640625\n",
      "Epoch 200000/500000 Cost: 3764.7109375\n",
      "Epoch 300000/500000 Cost: 3764.510009765625\n",
      "Epoch 400000/500000 Cost: 3764.509521484375\n",
      "MSE: 4392.9892578125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(\"Compare coefficients. sklearn vs. mine\")\n",
    "print(pd.DataFrame({\"sklearn\":np.concatenate([[reg.intercept_],reg.coef_.flatten()]), \"mine\":theta.detach().numpy()}))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Compare coefficients. sklearn vs. mine\n",
      "      sklearn        mine\n",
      "0  151.812188  151.811111\n",
      "1    2.334106    2.425052\n",
      "2  938.427960  937.117493\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression with Pytorch 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 1. load data\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "X = X[:,1:3]\n",
    "# X = np.concatenate([np.ones(X.shape[0]).reshape(-1,1), X], axis=1)\n",
    "\n",
    "# 2. Split Train and Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=5\n",
    ")\n",
    "N = X_train.shape[0]\n",
    "D_ = X_train.shape[1]\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float().view(-1,1)\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float().view(-1,1)\n",
    "\n",
    "# 3. Initialize model\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(D_, 1) # bias term is automatically added.\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "epochs= 500000\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = LinearRegression()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 4. Fit model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "# 5. Predict\n",
    "y_pred = model(X_test)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred.detach().numpy())}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE: 4392.9892578125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(\"Compare coefficients. sklearn vs. mine\")\n",
    "theta1_ = model.linear.weight.detach().numpy().flatten() # get weigths from model.\n",
    "theta0 = model.linear.bias.detach().numpy().flatten() # get bias(intercept) weight from model.\n",
    "theta = np.concatenate([theta0,theta1_]) # concat weights\n",
    "print(pd.DataFrame({\"sklearn\":np.concatenate([[reg.intercept_],reg.coef_.flatten()]), \"mine\":theta}))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Compare coefficients. sklearn vs. mine\n",
      "      sklearn        mine\n",
      "0  151.812188  151.811111\n",
      "1    2.334106    2.425158\n",
      "2  938.427960  937.117493\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "likelihood1: https://jjangjjong.tistory.com/41  \n",
    "likelihood2: https://angeloyeo.github.io/2020/07/17/MLE.html  \n",
    "cost function: https://computer-nerd.tistory.com/5  \n",
    "Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation: https://allenkunle.me/deriving-ml-cost-functions-part1  \n",
    "Linear Regression Normality: https://stats.stackexchange.com/questions/327427/how-is-y-normally-distributed-in-linear-regression  \n",
    "gradient descent: https://mccormickml.com/2014/03/04/gradient-descent-derivation/  \n",
    "notatinos: https://humanunsupervised.github.io/humanunsupervised.com/topics/L2-linear-regression-multivariate.html  \n",
    "linear regression with pytorch1: https://wikidocs.net/53560  \n",
    "linear regression with pytorch2: https://medium.com/biaslyai/pytorch-linear-and-logistic-regression-models-5c5f0da2cb9  "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "250c32ece4d06cfbaa606ac1fa53dfb2f6f512101e7381009ea9545415be61b5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}