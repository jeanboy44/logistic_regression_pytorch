{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Model with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 20.86929218377065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. load data\n",
    "cols = [\"CRIM\",\n",
    "    \"ZN\",\n",
    "    \"INDUS\",\n",
    "    \"CHAS\",\n",
    "    \"NOX\",\n",
    "    \"RM\",\n",
    "    \"AGE\",\n",
    "    \"DIS\",\n",
    "    \"RAD\",\n",
    "    \"TAX\",\n",
    "    \"PTRATIO\",\n",
    "    \"B\",\n",
    "    \"LSTAT\",\n",
    "    \"MEDV\",\n",
    "]\n",
    "# https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/code/datasets/housing/housing.data\n",
    "df = pd.read_csv(\n",
    "    \"../data/raw/housing.data.txt\",\n",
    "    delimiter=r\"\\s+\",\n",
    "    names=cols,\n",
    ")\n",
    "df = df.dropna()\n",
    "X = df.drop(\"MEDV\", axis=1)\n",
    "y = df[[\"MEDV\"]]\n",
    "\n",
    "# 2. Split Train and Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=5\n",
    ")\n",
    "\n",
    "# 3. Fit model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# 4. Measure performane\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "### 1. Prediction\n",
    "### 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "## Mathematical formula\n",
    "$$\n",
    "\\begin{align}\n",
    "y=\\theta^Tx\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "h_{\\theta}(x)=\\theta^Tx\n",
    "\\end{align}\n",
    "$$\n",
    "- notations\n",
    "    - $\\theta$: weight vector(D x 1 dim.)\n",
    "    - **x**: input(1 row) vector(D x 1 dim.)\n",
    "    - D: the number of features + 1\n",
    "- 참고\n",
    "    - 보통 선형회귀식은 intercept 항을 따로 빼서 표기하기도 하지만, 이후 수식 전개 등을 쉽게 하기 위해 theta0가 intercept라고 둠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = X_train.shape[0]\n",
    "D = X_train.shape[1]+1\n",
    "theta = np.random.normal(0, 1, D).reshape(-1,1)\n",
    "X = np.concatenate([np.ones(X_train.shape[0]).reshape(-1,1), X_train], axis=1) # Add bias term\n",
    "x = X_train.values[0]\n",
    "x = np.concatenate([[1], x]).reshape(-1,1) # add 1 for intercept \n",
    "\n",
    "# using for loop\n",
    "prediction = 0\n",
    "for i in range(D):\n",
    "    prediction += theta[i][0]*x[i][0]\n",
    "    prediction = prediction\n",
    "\n",
    "# vectorized\n",
    "prediction = theta.T.dot(x)\n",
    "\n",
    "# Predict multiple data\n",
    "predictions = X.dot(theta)\n",
    "\n",
    "def predict(X, theta):\n",
    "    X = np.concatenate([np.ones(X.shape[0]).reshape(-1,1), X], axis=1) # Add bias term\n",
    "    y_hat = X.dot(theta)\n",
    "    \n",
    "    return y_hat.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Cost function(= Loss function)\n",
    "### Cost function of linear regression: Mean Squared Error\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{N}\\sum_{i=1}^N(h_{\\theta}(x^{(i)})-y^{(i)})^2\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{N}\\sum_{i=1}^N((\\theta^Tx^{(i)}+b)-y^{(i)})^2\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{N}(X\\theta-y)^T(X\\theta-y)\n",
    "\\end{align}\n",
    "$$\n",
    "- notations\n",
    "    - **$\\theta$**: weight vector(D x 1 dim.)\n",
    "    - **x**: input(1 row) vector(D x 1 dim.)\n",
    "    - **X**: input matrix(N x D dim.)\n",
    "    - **y**: output vector(N x 1 dim.)\n",
    "    - D: the number of features\n",
    "- 위 세 개는 전부 같은 식\n",
    "- MAE를 최소가 되게 하는 **w**를 찾는다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def mse(y_hat, y):\n",
    "    N = len(y)\n",
    "    cost = sum((y_hat-y)**2)/N\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Method\n",
    "- 위에 정의한 cost function을 최대화 하기위해서는 어떠한 과정이 필요한가?\n",
    "- 선형회귀의 경우, 미분을 사용해서 해를 구할 수 있음!\n",
    "- 단, 일반적으로는 수리적인 방법(Ex. 동전던지기의 확률을 구한 방법, 미분 등)으로 해를 구할 수 없음\n",
    "- Optimization Method의 대표적인 예: **Gradient descent**\n",
    "    - Mathematical Formula\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta \\leftarrow \\theta - \\eta{\\nabla}_{\\theta}J\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\nabla}_{\\theta}J=\\sum_{n=1}^Nx^{(i)}(h_{\\theta}(x^{(i)})-y^{(i)})\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\nabla}_{\\theta}J=\\frac{1}{N}X^T(X\\theta-y)\n",
    "\\end{align}\n",
    "$$\n",
    "</br>\n",
    "$$\n",
    "h_{\\theta}(x^{(i)}) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2+...+ \\theta_Dx_D\n",
    "$$\n",
    "\n",
    "- notations\n",
    "    - $\\eta$ : learning rate\n",
    " \n",
    "<img src=\"../figures/GradientDescentGIF.gif\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "--------------------\n",
    "\n",
    "## 조금 더 깊게\n",
    "\n",
    "- 기계학습의 “학습”은 단순히 모델의 가중치(w)를 찾아내는 것\n",
    "    - 비유하자면, 새로운 기억이 생성될 때마다, 뇌에 있는 각 시냅스 간의 연결의 세기가 변한다!\n",
    "- 이러한 관점에서, 기계학습 문제는 단순히 주어진 데이터(X, y)를 가장 잘 설명하는 가중치를 찾아내는 것이다.\n",
    "- 이러한 가중치를 찾아내는 방법 중 가장 많이 사용되는 것이 최대우도추정(Maximum likelihood Estimation) 방법이다. \n",
    "\n",
    "### Base theorem\n",
    "![basetherom](../figures/baise_theorem.png)\n",
    "\n",
    "### Likelihood?\n",
    "<!-- ![likelihoood](../figures/likelihood2.png) -->\n",
    "<img src=\"../figures/likelihood2.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent 함수 생성\n",
    "def gradientDescent(X, y, theta, alpha, N, numIterations, verbose=1):\n",
    "    X_tmp = X.copy()\n",
    "    X_tmp = np.concatenate([np.ones(X_tmp.shape[0]).reshape(-1,1), X_tmp], axis=1) # Add bias term\n",
    "    X_tmp = X_tmp/100 # standardization for computing convinience\n",
    "    # bias 추가\n",
    "    for i in range(0, numIterations):\n",
    "        # Predict\n",
    "        hypothesis = X_tmp.dot(theta)\n",
    "        loss = hypothesis.reshape(-1,1) - y.reshape(-1,1)\n",
    "        # avg cost per example (the 2 in 2*n doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / N\n",
    "        if verbose==1:\n",
    "            if(i%10000==0):\n",
    "                print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = X_tmp.T.dot(loss) / N\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "X = X_train.values\n",
    "y = y_train.values.flatten()\n",
    "N, D = np.shape(X)\n",
    "numIterations= 30000000\n",
    "alpha = 0.05\n",
    "theta = np.random.normal(0, 1, D+1).reshape(-1,1) #Initalize theta\n",
    "theta = gradientDescent(X, y, theta, alpha, N, numIterations, verbose=0)\n",
    "# print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 22.758627549197097\n"
     ]
    }
   ],
   "source": [
    "def predict(X, theta):\n",
    "    X = np.concatenate([np.ones(X.shape[0]).reshape(-1,1), X], axis=1) # Add bias term\n",
    "    X = X/100\n",
    "    y_hat = X.dot(theta)\n",
    "    \n",
    "    return y_hat.flatten()\n",
    "# vectorized\n",
    "y_pred = predict(X_test.values, theta)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "likelihood1: https://jjangjjong.tistory.com/41  \n",
    "likelihood2: https://angeloyeo.github.io/2020/07/17/MLE.html  \n",
    "cost function: https://computer-nerd.tistory.com/5  \n",
    "Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation: https://allenkunle.me/deriving-ml-cost-functions-part1  \n",
    "Linear Regression Normality: https://stats.stackexchange.com/questions/327427/how-is-y-normally-distributed-in-linear-regression  \n",
    "gradient descent: https://mccormickml.com/2014/03/04/gradient-descent-derivation/  \n",
    "notatinos: https://humanunsupervised.github.io/humanunsupervised.com/topics/L2-linear-regression-multivariate.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "250c32ece4d06cfbaa606ac1fa53dfb2f6f512101e7381009ea9545415be61b5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
